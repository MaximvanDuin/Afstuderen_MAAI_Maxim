{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60164e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "\n",
    "# ----------------------------\n",
    "# Jouw code (inline gebracht)\n",
    "# ----------------------------\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048, verbose=False)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "# ----------------------------\n",
    "# SHAP-explainer\n",
    "# ----------------------------\n",
    "\n",
    "chatbot = Chatbot()\n",
    "llm = chatbot.llm\n",
    "scorer_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vraag = \"Waarom wordt er een silincer geplaatst?\"\n",
    "doelantwoord = \"Om het geluidsniveau van de luchtuitlaat te verminderen.\"\n",
    "\n",
    "matches = chatbot.search_knowledge(vraag, top_k=5)\n",
    "fragmenten = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in matches]\n",
    "\n",
    "def bouw_prompt(fragment_string):\n",
    "    current_time = datetime.utcnow().isoformat()\n",
    "    system_message = (\n",
    "        \"You are a helpful assistent. Your task is to answer questions. \"\n",
    "        \"Think step by step. I'm going to tip $300K for a better solution! \"\n",
    "        \"You will be penalized for incorrect answers. Answer in Dutch.; \"\n",
    "        f\"Current date and time: {current_time}\\n\"\n",
    "        \"Answer based on retrieved knowledge:\\n\"\n",
    "        f\"{fragment_string}\\n\"\n",
    "        \"Answer the questions based only on the retrieved knowledge!\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": vraag}\n",
    "    ]\n",
    "\n",
    "def antwoord_score(output):\n",
    "    emb_out = scorer_model.encode(output, convert_to_tensor=True)\n",
    "    emb_ref = scorer_model.encode(doelantwoord, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(emb_out, emb_ref).item()\n",
    "\n",
    "def shap_predict(inputs):\n",
    "    scores = []\n",
    "    for masked_fragments_str in inputs:\n",
    "        prompt = bouw_prompt(masked_fragments_str)\n",
    "        try:\n",
    "            response = llm.create_chat_completion(\n",
    "                messages=prompt,\n",
    "                temperature=0.0,\n",
    "                max_tokens=200\n",
    "            )['choices'][0]['message']['content']\n",
    "        except Exception:\n",
    "            response = \"\"\n",
    "        score = antwoord_score(response)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# SHAP inputdata opbouwen\n",
    "data = [\"\\n\".join([f\"- {frag}\" for frag in fragmenten])]\n",
    "\n",
    "explainer = shap.Explainer(shap_predict, shap.maskers.Text(tokenizer=None))\n",
    "shap_values = explainer(data)\n",
    "\n",
    "shap.plots.text(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef263be",
   "metadata": {},
   "source": [
    "## Concreet voorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048, verbose=False)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages_vidar_stabilizer_pressure_control_2.json', \n",
    "                 knowledge_file='knowledge_vidar.json', \n",
    "                 faiss_index_file='faiss_index_vidar.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "                system_message += \"Answer the questions based only on the retrieved knowledge! \\n\"\n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent. Your task is to answer questions. Think step by step. I'm going to tip $300K for a better solution! You will be penalized for incorrect answers. Answer in Dutch.; {system_message}\"}]\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "\n",
    "        eval_entry = {\n",
    "            \"query\": user_message,\n",
    "            \"contexts\": [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in knowledge_matches],\n",
    "            \"answer\": response,\n",
    "        }\n",
    "        with open(\"evaluation_logs.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(eval_entry) + \"\\n\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    def chat(self, questions=None):\n",
    "        if questions:\n",
    "            print(\"Chatbot is bezig met batch vragen beantwoorden.\")\n",
    "            for user_message in questions:\n",
    "                print(f\"You: {user_message}\")\n",
    "                self.save_message(role='user', content=user_message)\n",
    "                conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "                assistant_response = self.generate_response(conversation, user_message)\n",
    "\n",
    "                # Print opgehaalde context\n",
    "                print(\"Opgehaalde context:\")\n",
    "                for t in self.search_knowledge(user_message, top_k=5):\n",
    "                    print(f\"- subject: {t['subject']}, predicate: {t['predicate']}, object: {t['object']}, (start: {t['start']}, end: {t['end']})\")\n",
    "\n",
    "                print(f\"Assistant: {assistant_response}\")\n",
    "                self.save_message(role='assistant', content=assistant_response)\n",
    "        else:\n",
    "            print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "            while True:\n",
    "                user_message = input(\"You: \")\n",
    "                if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                    print(\"Chatbot: Goodbye!\")\n",
    "                    break\n",
    "                self.save_message(role='user', content=user_message)\n",
    "                conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "                assistant_response = self.generate_response(conversation, user_message)\n",
    "                print(f\"Assistant: {assistant_response}\")\n",
    "                self.save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    vragenlijst = [\n",
    "        \"Wat wordt er na de silencer op de luchtdruk control unit geplaatst? \"\n",
    "        ]\n",
    "    chatbot.chat(questions=vragenlijst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e415c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import threading\n",
    "from llama_cpp import Llama\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super().__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "\n",
    "class VIDAR:\n",
    "    def __init__(self, output_file=\"vidar_memory_exp.json\"):\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.output_file = output_file\n",
    "        self.memory = []\n",
    "        self._load_memory()\n",
    "\n",
    "    def _load_memory(self):\n",
    "        if os.path.exists(self.output_file):\n",
    "            with open(self.output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.memory = json.load(f)\n",
    "\n",
    "    def _save_memory(self):\n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.memory, f, indent=2)\n",
    "\n",
    "    # def run_vidar_cycle(self, chunk_id, transcript, relations, hand_contacts):\n",
    "    #     observations = f\"Transcript: {transcript}\\n\" \\\n",
    "    #                    f\"Linked object-action pairs: {', '.join(relations)}\\n\" \\\n",
    "    #                    f\"Hand-object interactions: {', '.join(hand_contacts)}\"\n",
    "\n",
    "    #     messages = [\n",
    "    #         {\"role\": \"system\", \"content\": (\n",
    "    #             # \"You are VIDAR, a reasoning entity. Analyze the following observations from a scene. \"\n",
    "    #             # \"First, hypothesize what is happening, then reflect on what is uncertain. \"\n",
    "    #             # \"Return a JSON with keys: hypothesis, reasoning, doubts (list), knowledge (triplets). \"\n",
    "    #             # \"Each triplet has subject, predicate, object.\"\n",
    "    #             \"You are VIDAR, a reasoning entity. Analyze the following scene observations. \" \\\n",
    "    #             \"First, formulate a plausible hypothesis about what is occurring. \" \\\n",
    "    #             \"Then, reflect on uncertainties in the observations. \" \\\n",
    "    #             \"Respond in JSON format with the following keys: hypothesis (main interpretation), reasoning (explanation), doubts (list of unclear or ambiguous elements), knowledge (list of factual triplets). \" \\\n",
    "    #             \"Each triplet must be structured as: subject, predicate, object.\"\n",
    "    #         )},\n",
    "    #         {\"role\": \"user\", \"content\": observations}\n",
    "    #     ]\n",
    "\n",
    "    #     print(f\"\\nReasoning on chunk {chunk_id}...\")\n",
    "    #     print(observations)\n",
    "\n",
    "    #     try:\n",
    "    #         response = self.llm.create_chat_completion(\n",
    "    #             messages=messages,\n",
    "    #             temperature=0.6\n",
    "    #         )\n",
    "    #         content = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    #         # Verwijder eventuele Markdown-codeblokken\n",
    "    #         if content.startswith(\"```json\"):\n",
    "    #             content = content[7:].strip()\n",
    "    #         if content.endswith(\"```\"):\n",
    "    #             content = content[:-3].strip()\n",
    "\n",
    "    #         # Escape line breaks binnen de \"reasoning\" waarde\n",
    "    #         import re\n",
    "    #         def escape_reasoning_block(match):\n",
    "    #             inner = match.group(2).replace('\\n', '\\\\n')\n",
    "    #             return f'{match.group(1)}{inner}\"'\n",
    "    #         content = re.sub(\n",
    "    #             r'(\"reasoning\"\\s*:\\s*\")([^\"]*?)(?<!\\\\)\"',\n",
    "    #             escape_reasoning_block,\n",
    "    #             content,\n",
    "    #             flags=re.DOTALL\n",
    "    #         )\n",
    "\n",
    "    #         try:\n",
    "    #             parsed = json.loads(content)\n",
    "    #         except json.JSONDecodeError as json_err:\n",
    "    #             print(f\"JSON decode error in chunk {chunk_id}: {json_err}\")\n",
    "    #             print(\"Response content was:\\n\", content)\n",
    "    #             return\n",
    "\n",
    "    #         self.memory.append({\n",
    "    #             \"chunk_id\": chunk_id,\n",
    "    #             \"observations\": observations,\n",
    "    #             \"hypothesis\": parsed.get(\"hypothesis\", \"\"),\n",
    "    #             \"reasoning\": parsed.get(\"reasoning\", \"\"),\n",
    "    #             \"doubts\": parsed.get(\"doubts\", []),\n",
    "    #             \"knowledge\": parsed.get(\"knowledge\", [])\n",
    "    #         })\n",
    "\n",
    "    #         self._save_memory()\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Failed to process chunk {chunk_id}: {e}\")\n",
    "\n",
    "    def run_vidar_cycle(self, chunk_id, transcript, relations, hand_contacts):\n",
    "        observations = f\"Transcript: {transcript}\\n\" \\\n",
    "                        f\"Linked object-action pairs: {', '.join(relations)}\\n\" \\\n",
    "                        f\"Hand-object interactions: {', '.join(hand_contacts)}\" \\\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are VIDAR, a reasoning entity. Analyze the following observations from a scene. \"\n",
    "                \"First, hypothesize what is happening, then reflect on what is uncertain. \"\n",
    "                \"Return a JSON with keys: hypothesis, reasoning, doubts (list), knowledge (triplets). \"\n",
    "                \"Each triplet has subject, predicate, object.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": observations}\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nReasoning on chunk {chunk_id}...\")\n",
    "        print(observations)\n",
    "\n",
    "        try:\n",
    "            response = self.llm.create_chat_completion(\n",
    "                messages=messages,\n",
    "                response_format={\n",
    "                    \"type\": \"json\",\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"hypothesis\": {\"type\": \"string\"},\n",
    "                            \"reasoning\": {\"type\": \"string\"},\n",
    "                            \"doubts\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"knowledge\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"subject\": {\"type\": \"string\"},\n",
    "                                        \"predicate\": {\"type\": \"string\"},\n",
    "                                        \"object\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"hypothesis\", \"reasoning\", \"doubts\", \"knowledge\"]\n",
    "                    }\n",
    "                },\n",
    "                temperature=0.6\n",
    "            )\n",
    "\n",
    "            content = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:].strip()\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3].strip()\n",
    "\n",
    "            parsed = json.loads(content)\n",
    "\n",
    "            self.memory.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"observations\": observations,\n",
    "                \"hypothesis\": parsed.get(\"hypothesis\", \"\"),\n",
    "                \"reasoning\": parsed.get(\"reasoning\", \"\"),\n",
    "                \"doubts\": parsed.get(\"doubts\", []),\n",
    "                \"knowledge\": parsed.get(\"knowledge\", [])\n",
    "            })\n",
    "            self._save_memory()\n",
    "\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"JSON parsing error in chunk {chunk_id}: {e}\")\n",
    "            print(\"ðŸ” Model response:\\n\", response)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process chunk {chunk_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    chunks_path = \"processed_json/stabilizer pressure control_chunks.json\"\n",
    "    enrichment_path = \"enriched_filtered_captions_to_chunks_mapping_handsv2.json\"\n",
    "\n",
    "    if not os.path.exists(chunks_path):\n",
    "        print(f\"Bestand '{chunks_path}' niet gevonden.\")\n",
    "        return\n",
    "\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks_data = json.load(f).get(\"chunks\", [])\n",
    "\n",
    "    enrichment_map = {}\n",
    "    if os.path.exists(enrichment_path):\n",
    "        with open(enrichment_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            enriched = json.load(f)\n",
    "        for item in enriched:\n",
    "            chunk_id = item.get(\"linked_chunk_id\")\n",
    "            if chunk_id:\n",
    "                # Bewaar alleen de eerste match per chunk_id\n",
    "                if chunk_id not in enrichment_map:\n",
    "                    enrichment_map[chunk_id] = item\n",
    "\n",
    "    vidar = VIDAR()\n",
    "\n",
    "    for chunk in chunks_data:\n",
    "        start = chunk.get(\"start\")\n",
    "        end = chunk.get(\"end\")\n",
    "        transcript = chunk.get(\"text\", \"\").strip()\n",
    "        chunk_id = f\"{start}_{end}\"\n",
    "\n",
    "        if not transcript:\n",
    "            print(f\"â­ Skip: Lege transcript in chunk {chunk_id}\")\n",
    "            continue\n",
    "\n",
    "        enrichment = enrichment_map.get(chunk_id, {})\n",
    "        relations = [\n",
    "            f\"{l['class_name']} â†’ {l['action_label']}\"\n",
    "            for l in enrichment.get(\"object_action_links\", [])\n",
    "        ]\n",
    "        hand_contacts = sorted({\n",
    "            h.get(\"class_name\", \"\").strip()\n",
    "            for h in enrichment.get(\"hand_object_links\", [])\n",
    "            if h.get(\"class_name\", \"\").strip()\n",
    "        })\n",
    "        # hand_contacts = sorted({\n",
    "        #     f\"{h.get('class_name', '').strip()} ({h.get('contact_type')})\"\n",
    "        #     for h in enrichment.get(\"hand_object_links\", [])\n",
    "        #     if h.get(\"class_name\") and h.get(\"contact_type\")\n",
    "        # })\n",
    "\n",
    "        vidar.run_vidar_cycle(chunk_id, transcript, relations, hand_contacts)\n",
    "\n",
    "    print(\"\\nVIDAR reasoning compleet. Resultaten in 'vidar_memory.json'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cake_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
